<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dora.conf API documentation</title>
<meta name="description" content="Basic configuration for Dora is here." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dora.conf</code></h1>
</header>
<section id="section-intro">
<p>Basic configuration for Dora is here.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

&#34;&#34;&#34;
Basic configuration for Dora is here.
&#34;&#34;&#34;
from argparse import Namespace
from dataclasses import dataclass, field
from fnmatch import fnmatch
from pathlib import Path
import typing as tp

from omegaconf.dictconfig import DictConfig
from omegaconf import OmegaConf


def update_from_args(data: tp.Any, args: Namespace):
    &#34;&#34;&#34;Update the given dataclass from the argument parser args.
    &#34;&#34;&#34;
    for key in data.__dict__:
        assert isinstance(key, str)
        if hasattr(args, key):
            value = getattr(args, key)
            if value is not None:
                setattr(data, key, value)


def update_from_hydra(data: tp.Any, cfg: DictConfig):
    &#34;&#34;&#34;Update the given dataclass from the hydra config.
    &#34;&#34;&#34;

    dct = OmegaConf.to_container(cfg, resolve=True)
    assert isinstance(dct, dict)
    for key, value in dct.items():
        assert isinstance(key, str)
        if hasattr(data, key):
            setattr(data, key, value)
        else:
            raise AttributeError(f&#34;Object of type {data.__class__} &#34;
                                 f&#34;does not have an attribute {key}&#34;)


@dataclass
class SlurmConfig:
    &#34;&#34;&#34;
    Configuration when scheduling a job.
    This differs slightly from Slurm/Submitit params because this will
    automatically scale some values based on the number of GPUs.

    Args:
        gpus (int): number of total GPUs to schedule. Number of nodes
            and tasks per nodes will be automatically inferred.
        mem_per_gpu (float): amount of memory in GB to schedule
            per gpus.
        time (int): maximum duration for the job in minutes.
        cpus_per_gpu (int): number of cpus per gpu, this will set
            the `cpus_per_task` automatically, based on the
            number of gpus and `one_task_per_node`, unless `cpus_per_task`
            is explicitely provided.
        cpus_per_task (int or None): number of cpus per task.
        partition (str): partition name
        comment (str): comment for the job.
        setup (List[str]): list of shell commands to execute
            before the actual command. Use it for `module load`.
        max_num_timeout (int): maximum number of requeue.
        one_task_per_node (bool): if True, schedules a single task
            per node, otherwise, will schedule one task per gpu (default is False).
        array_parallelism (int): when using job arrays, how many tasks can run
            in parallel.
        qos (str or None): qos param for slurm.
        account (str or None): account param for slurm.
        dependents (int): if &gt; 0, start a number of dependent jobs. Requeuing
            will be deactivated and rely on dependent jobs instead.

    ..warning:: this assumes one task per GPU.
        Set `one_task_per_node` if you do not want that.
        Tasks without any gpus are not really supported at the moment.
    &#34;&#34;&#34;
    gpus: int = 1
    mem_per_gpu: float = 40
    time: int = 1200
    cpus_per_gpu: int = 10
    cpus_per_task: tp.Optional[int] = None
    partition: str = &#34;learnlab&#34;
    comment: tp.Optional[str] = None
    setup: tp.List[str] = field(default_factory=list)
    max_num_timeout: int = 20
    constraint: str = &#34;&#34;
    one_task_per_node: bool = False
    array_parallelism: int = 256
    exclude: tp.Optional[str] = None
    qos: tp.Optional[str] = None
    account: tp.Optional[str] = None
    dependents: int = 0


@dataclass
class SubmitRules:
    &#34;&#34;&#34;
    Submit rules describe in which case Shepherd will schedule new jobs.

    Args:
        retry (bool): if true, all failed or canceled jobs will be rescheduled.
        update_pending (bool): if true, all pending jobs whose Slurm parameters
            have changed will be replaced.
        update (bool): if true, all pending or running jobs whose Slurm parameters
            have changed will be replaced.
        replace (bool): if true, all running jobs will be replaced by new jobs.
        replace_done (bool): if true, all done jobs will be relaunched.
    &#34;&#34;&#34;

    retry: bool = False
    update: bool = False
    replace: bool = False
    replace_done: bool = False


@dataclass
class ShepConfig:
    &#34;&#34;&#34;
    Configuration for Shepherd. Mostly naming conventions for folders and files.
    There should be little reasons to change that.
    &#34;&#34;&#34;
    job_file: str = &#34;job.pkl&#34;
    by_id: str = &#34;by_id&#34;
    orphans: str = &#34;orphans&#34;
    submitit_folder: str = &#34;submitit&#34;
    latest_submitit: str = &#34;latest&#34;
    arrays: str = &#34;arrays&#34;


@dataclass
class DoraConfig:
    &#34;&#34;&#34;
    Main Dora configuration. The main parameters to change are the following.

    Args:
        dir (Path or str): path where Dora will save all useful informations, logs.
            This is also where you should store your checkpoints (see `dora.xp.XP`).
        exclude (List[str]): list of patterns of argument names to ignore
            when computing the XP signature and doing deduplication.
            For instance &#39;num_workers&#39;, etc.
        git_save (bool): when True, experiments can only be scheduled from a clean repo.
            A shallow clone of the repo will be made and execution will happen from there.
            This does not impact `dora run` unless you pass the `--git_save` flag.
        shared (Path or None): if provided, the path to a central repository of XPs.
            For the moment, this only supports sharing hyper-params, logs etc. will stay
            in the per user folder.
        grid_package (str or None): if provided, package to look for grids. Default
            to the package with the `train.py` module followed by `.grids`.
    &#34;&#34;&#34;
    dir: Path = Path(&#34;./outputs&#34;)  # where everything will be stored
    exclude: tp.List[str] = field(default_factory=list)
    git_save: bool = False
    shared: tp.Optional[Path] = None  # Optional path for shared XPs.
    grid_package: tp.Optional[str] = None

    # Those are internal config values and are unlikely to be changed
    history: str = &#34;history.json&#34;  # where metrics will be stored
    xps: str = &#34;xps&#34;  # subfolder to store xps

    shep: ShepConfig = field(default_factory=ShepConfig)
    rendezvous_file: str = &#34;rendezvous.txt&#34;
    use_rendezvous: bool = False
    # Filenames used in various places, you shouldn&#39;t edit that
    _grids: str = &#34;grids&#34;
    _codes: str = &#34;codes&#34;

    def is_excluded(self, arg_name: str) -&gt; bool:
        &#34;&#34;&#34;Return True if the given argument name should be excluded from
        the signature.&#34;&#34;&#34;
        for pattern in self.exclude:
            if fnmatch(arg_name, pattern):
                return True
        return False

    def __setattr__(self, name, value):
        if name in [&#39;dir&#39;, &#39;shared&#39;]:
            from .git_save import to_absolute_path
            if value is not None:
                value = Path(to_absolute_path(value))
        super().__setattr__(name, value)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dora.conf.update_from_args"><code class="name flex">
<span>def <span class="ident">update_from_args</span></span>(<span>data: Any, args: argparse.Namespace)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the given dataclass from the argument parser args.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_from_args(data: tp.Any, args: Namespace):
    &#34;&#34;&#34;Update the given dataclass from the argument parser args.
    &#34;&#34;&#34;
    for key in data.__dict__:
        assert isinstance(key, str)
        if hasattr(args, key):
            value = getattr(args, key)
            if value is not None:
                setattr(data, key, value)</code></pre>
</details>
</dd>
<dt id="dora.conf.update_from_hydra"><code class="name flex">
<span>def <span class="ident">update_from_hydra</span></span>(<span>data: Any, cfg: omegaconf.dictconfig.DictConfig)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the given dataclass from the hydra config.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_from_hydra(data: tp.Any, cfg: DictConfig):
    &#34;&#34;&#34;Update the given dataclass from the hydra config.
    &#34;&#34;&#34;

    dct = OmegaConf.to_container(cfg, resolve=True)
    assert isinstance(dct, dict)
    for key, value in dct.items():
        assert isinstance(key, str)
        if hasattr(data, key):
            setattr(data, key, value)
        else:
            raise AttributeError(f&#34;Object of type {data.__class__} &#34;
                                 f&#34;does not have an attribute {key}&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dora.conf.DoraConfig"><code class="flex name class">
<span>class <span class="ident">DoraConfig</span></span>
<span>(</span><span>dir: pathlib.Path = PosixPath('outputs'), exclude: List[str] = &lt;factory&gt;, git_save: bool = False, shared: Optional[pathlib.Path] = None, grid_package: Optional[str] = None, history: str = 'history.json', xps: str = 'xps', shep: <a title="dora.conf.ShepConfig" href="#dora.conf.ShepConfig">ShepConfig</a> = &lt;factory&gt;, rendezvous_file: str = 'rendezvous.txt', use_rendezvous: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Main Dora configuration. The main parameters to change are the following.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dir</code></strong> :&ensp;<code>Path</code> or <code>str</code></dt>
<dd>path where Dora will save all useful informations, logs.
This is also where you should store your checkpoints (see <code><a title="dora.xp.XP" href="xp.html#dora.xp.XP">XP</a></code>).</dd>
<dt><strong><code>exclude</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>list of patterns of argument names to ignore
when computing the XP signature and doing deduplication.
For instance 'num_workers', etc.</dd>
<dt><strong><code>git_save</code></strong> :&ensp;<code>bool</code></dt>
<dd>when True, experiments can only be scheduled from a clean repo.
A shallow clone of the repo will be made and execution will happen from there.
This does not impact <code><a title="dora" href="index.html">dora</a> run</code> unless you pass the <code>--git_save</code> flag.</dd>
<dt><strong><code>shared</code></strong> :&ensp;<code>Path</code> or <code>None</code></dt>
<dd>if provided, the path to a central repository of XPs.
For the moment, this only supports sharing hyper-params, logs etc. will stay
in the per user folder.</dd>
<dt><strong><code>grid_package</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>if provided, package to look for grids. Default
to the package with the <code>train.py</code> module followed by <code>.grids</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DoraConfig:
    &#34;&#34;&#34;
    Main Dora configuration. The main parameters to change are the following.

    Args:
        dir (Path or str): path where Dora will save all useful informations, logs.
            This is also where you should store your checkpoints (see `dora.xp.XP`).
        exclude (List[str]): list of patterns of argument names to ignore
            when computing the XP signature and doing deduplication.
            For instance &#39;num_workers&#39;, etc.
        git_save (bool): when True, experiments can only be scheduled from a clean repo.
            A shallow clone of the repo will be made and execution will happen from there.
            This does not impact `dora run` unless you pass the `--git_save` flag.
        shared (Path or None): if provided, the path to a central repository of XPs.
            For the moment, this only supports sharing hyper-params, logs etc. will stay
            in the per user folder.
        grid_package (str or None): if provided, package to look for grids. Default
            to the package with the `train.py` module followed by `.grids`.
    &#34;&#34;&#34;
    dir: Path = Path(&#34;./outputs&#34;)  # where everything will be stored
    exclude: tp.List[str] = field(default_factory=list)
    git_save: bool = False
    shared: tp.Optional[Path] = None  # Optional path for shared XPs.
    grid_package: tp.Optional[str] = None

    # Those are internal config values and are unlikely to be changed
    history: str = &#34;history.json&#34;  # where metrics will be stored
    xps: str = &#34;xps&#34;  # subfolder to store xps

    shep: ShepConfig = field(default_factory=ShepConfig)
    rendezvous_file: str = &#34;rendezvous.txt&#34;
    use_rendezvous: bool = False
    # Filenames used in various places, you shouldn&#39;t edit that
    _grids: str = &#34;grids&#34;
    _codes: str = &#34;codes&#34;

    def is_excluded(self, arg_name: str) -&gt; bool:
        &#34;&#34;&#34;Return True if the given argument name should be excluded from
        the signature.&#34;&#34;&#34;
        for pattern in self.exclude:
            if fnmatch(arg_name, pattern):
                return True
        return False

    def __setattr__(self, name, value):
        if name in [&#39;dir&#39;, &#39;shared&#39;]:
            from .git_save import to_absolute_path
            if value is not None:
                value = Path(to_absolute_path(value))
        super().__setattr__(name, value)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="dora.conf.DoraConfig.dir"><code class="name">var <span class="ident">dir</span> : pathlib.Path</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.DoraConfig.exclude"><code class="name">var <span class="ident">exclude</span> : List[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.DoraConfig.git_save"><code class="name">var <span class="ident">git_save</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.DoraConfig.grid_package"><code class="name">var <span class="ident">grid_package</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.DoraConfig.history"><code class="name">var <span class="ident">history</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.DoraConfig.rendezvous_file"><code class="name">var <span class="ident">rendezvous_file</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.DoraConfig.shared"><code class="name">var <span class="ident">shared</span> : Optional[pathlib.Path]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.DoraConfig.shep"><code class="name">var <span class="ident">shep</span> : <a title="dora.conf.ShepConfig" href="#dora.conf.ShepConfig">ShepConfig</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.DoraConfig.use_rendezvous"><code class="name">var <span class="ident">use_rendezvous</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.DoraConfig.xps"><code class="name">var <span class="ident">xps</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="dora.conf.DoraConfig.is_excluded"><code class="name flex">
<span>def <span class="ident">is_excluded</span></span>(<span>self, arg_name: str) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Return True if the given argument name should be excluded from
the signature.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_excluded(self, arg_name: str) -&gt; bool:
    &#34;&#34;&#34;Return True if the given argument name should be excluded from
    the signature.&#34;&#34;&#34;
    for pattern in self.exclude:
        if fnmatch(arg_name, pattern):
            return True
    return False</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="dora.conf.ShepConfig"><code class="flex name class">
<span>class <span class="ident">ShepConfig</span></span>
<span>(</span><span>job_file: str = 'job.pkl', by_id: str = 'by_id', orphans: str = 'orphans', submitit_folder: str = 'submitit', latest_submitit: str = 'latest', arrays: str = 'arrays')</span>
</code></dt>
<dd>
<div class="desc"><p>Configuration for Shepherd. Mostly naming conventions for folders and files.
There should be little reasons to change that.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ShepConfig:
    &#34;&#34;&#34;
    Configuration for Shepherd. Mostly naming conventions for folders and files.
    There should be little reasons to change that.
    &#34;&#34;&#34;
    job_file: str = &#34;job.pkl&#34;
    by_id: str = &#34;by_id&#34;
    orphans: str = &#34;orphans&#34;
    submitit_folder: str = &#34;submitit&#34;
    latest_submitit: str = &#34;latest&#34;
    arrays: str = &#34;arrays&#34;</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="dora.conf.ShepConfig.arrays"><code class="name">var <span class="ident">arrays</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.ShepConfig.by_id"><code class="name">var <span class="ident">by_id</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.ShepConfig.job_file"><code class="name">var <span class="ident">job_file</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.ShepConfig.latest_submitit"><code class="name">var <span class="ident">latest_submitit</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.ShepConfig.orphans"><code class="name">var <span class="ident">orphans</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.ShepConfig.submitit_folder"><code class="name">var <span class="ident">submitit_folder</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="dora.conf.SlurmConfig"><code class="flex name class">
<span>class <span class="ident">SlurmConfig</span></span>
<span>(</span><span>gpus: int = 1, mem_per_gpu: float = 40, time: int = 1200, cpus_per_gpu: int = 10, cpus_per_task: Optional[int] = None, partition: str = 'learnlab', comment: Optional[str] = None, setup: List[str] = &lt;factory&gt;, max_num_timeout: int = 20, constraint: str = '', one_task_per_node: bool = False, array_parallelism: int = 256, exclude: Optional[str] = None, qos: Optional[str] = None, account: Optional[str] = None, dependents: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Configuration when scheduling a job.
This differs slightly from Slurm/Submitit params because this will
automatically scale some values based on the number of GPUs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gpus</code></strong> :&ensp;<code>int</code></dt>
<dd>number of total GPUs to schedule. Number of nodes
and tasks per nodes will be automatically inferred.</dd>
<dt><strong><code>mem_per_gpu</code></strong> :&ensp;<code>float</code></dt>
<dd>amount of memory in GB to schedule
per gpus.</dd>
<dt><strong><code>time</code></strong> :&ensp;<code>int</code></dt>
<dd>maximum duration for the job in minutes.</dd>
<dt><strong><code>cpus_per_gpu</code></strong> :&ensp;<code>int</code></dt>
<dd>number of cpus per gpu, this will set
the <code>cpus_per_task</code> automatically, based on the
number of gpus and <code>one_task_per_node</code>, unless <code>cpus_per_task</code>
is explicitely provided.</dd>
<dt><strong><code>cpus_per_task</code></strong> :&ensp;<code>int</code> or <code>None</code></dt>
<dd>number of cpus per task.</dd>
<dt><strong><code>partition</code></strong> :&ensp;<code>str</code></dt>
<dd>partition name</dd>
<dt><strong><code>comment</code></strong> :&ensp;<code>str</code></dt>
<dd>comment for the job.</dd>
<dt><strong><code>setup</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>list of shell commands to execute
before the actual command. Use it for <code>module load</code>.</dd>
<dt><strong><code>max_num_timeout</code></strong> :&ensp;<code>int</code></dt>
<dd>maximum number of requeue.</dd>
<dt><strong><code>one_task_per_node</code></strong> :&ensp;<code>bool</code></dt>
<dd>if True, schedules a single task
per node, otherwise, will schedule one task per gpu (default is False).</dd>
<dt><strong><code>array_parallelism</code></strong> :&ensp;<code>int</code></dt>
<dd>when using job arrays, how many tasks can run
in parallel.</dd>
<dt><strong><code>qos</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>qos param for slurm.</dd>
<dt><strong><code>account</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>account param for slurm.</dd>
<dt><strong><code>dependents</code></strong> :&ensp;<code>int</code></dt>
<dd>if &gt; 0, start a number of dependent jobs. Requeuing
will be deactivated and rely on dependent jobs instead.</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;this assumes one task per GPU.</p>
<p>Set <code>one_task_per_node</code> if you do not want that.
Tasks without any gpus are not really supported at the moment.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SlurmConfig:
    &#34;&#34;&#34;
    Configuration when scheduling a job.
    This differs slightly from Slurm/Submitit params because this will
    automatically scale some values based on the number of GPUs.

    Args:
        gpus (int): number of total GPUs to schedule. Number of nodes
            and tasks per nodes will be automatically inferred.
        mem_per_gpu (float): amount of memory in GB to schedule
            per gpus.
        time (int): maximum duration for the job in minutes.
        cpus_per_gpu (int): number of cpus per gpu, this will set
            the `cpus_per_task` automatically, based on the
            number of gpus and `one_task_per_node`, unless `cpus_per_task`
            is explicitely provided.
        cpus_per_task (int or None): number of cpus per task.
        partition (str): partition name
        comment (str): comment for the job.
        setup (List[str]): list of shell commands to execute
            before the actual command. Use it for `module load`.
        max_num_timeout (int): maximum number of requeue.
        one_task_per_node (bool): if True, schedules a single task
            per node, otherwise, will schedule one task per gpu (default is False).
        array_parallelism (int): when using job arrays, how many tasks can run
            in parallel.
        qos (str or None): qos param for slurm.
        account (str or None): account param for slurm.
        dependents (int): if &gt; 0, start a number of dependent jobs. Requeuing
            will be deactivated and rely on dependent jobs instead.

    ..warning:: this assumes one task per GPU.
        Set `one_task_per_node` if you do not want that.
        Tasks without any gpus are not really supported at the moment.
    &#34;&#34;&#34;
    gpus: int = 1
    mem_per_gpu: float = 40
    time: int = 1200
    cpus_per_gpu: int = 10
    cpus_per_task: tp.Optional[int] = None
    partition: str = &#34;learnlab&#34;
    comment: tp.Optional[str] = None
    setup: tp.List[str] = field(default_factory=list)
    max_num_timeout: int = 20
    constraint: str = &#34;&#34;
    one_task_per_node: bool = False
    array_parallelism: int = 256
    exclude: tp.Optional[str] = None
    qos: tp.Optional[str] = None
    account: tp.Optional[str] = None
    dependents: int = 0</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="dora.conf.SlurmConfig.account"><code class="name">var <span class="ident">account</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.array_parallelism"><code class="name">var <span class="ident">array_parallelism</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.comment"><code class="name">var <span class="ident">comment</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.constraint"><code class="name">var <span class="ident">constraint</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.cpus_per_gpu"><code class="name">var <span class="ident">cpus_per_gpu</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.cpus_per_task"><code class="name">var <span class="ident">cpus_per_task</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.dependents"><code class="name">var <span class="ident">dependents</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.exclude"><code class="name">var <span class="ident">exclude</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.gpus"><code class="name">var <span class="ident">gpus</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.max_num_timeout"><code class="name">var <span class="ident">max_num_timeout</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.mem_per_gpu"><code class="name">var <span class="ident">mem_per_gpu</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.one_task_per_node"><code class="name">var <span class="ident">one_task_per_node</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.partition"><code class="name">var <span class="ident">partition</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.qos"><code class="name">var <span class="ident">qos</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.setup"><code class="name">var <span class="ident">setup</span> : List[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SlurmConfig.time"><code class="name">var <span class="ident">time</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="dora.conf.SubmitRules"><code class="flex name class">
<span>class <span class="ident">SubmitRules</span></span>
<span>(</span><span>retry: bool = False, update: bool = False, replace: bool = False, replace_done: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Submit rules describe in which case Shepherd will schedule new jobs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>retry</code></strong> :&ensp;<code>bool</code></dt>
<dd>if true, all failed or canceled jobs will be rescheduled.</dd>
<dt><strong><code>update_pending</code></strong> :&ensp;<code>bool</code></dt>
<dd>if true, all pending jobs whose Slurm parameters
have changed will be replaced.</dd>
<dt><strong><code>update</code></strong> :&ensp;<code>bool</code></dt>
<dd>if true, all pending or running jobs whose Slurm parameters
have changed will be replaced.</dd>
<dt><strong><code>replace</code></strong> :&ensp;<code>bool</code></dt>
<dd>if true, all running jobs will be replaced by new jobs.</dd>
<dt><strong><code>replace_done</code></strong> :&ensp;<code>bool</code></dt>
<dd>if true, all done jobs will be relaunched.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SubmitRules:
    &#34;&#34;&#34;
    Submit rules describe in which case Shepherd will schedule new jobs.

    Args:
        retry (bool): if true, all failed or canceled jobs will be rescheduled.
        update_pending (bool): if true, all pending jobs whose Slurm parameters
            have changed will be replaced.
        update (bool): if true, all pending or running jobs whose Slurm parameters
            have changed will be replaced.
        replace (bool): if true, all running jobs will be replaced by new jobs.
        replace_done (bool): if true, all done jobs will be relaunched.
    &#34;&#34;&#34;

    retry: bool = False
    update: bool = False
    replace: bool = False
    replace_done: bool = False</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="dora.conf.SubmitRules.replace"><code class="name">var <span class="ident">replace</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SubmitRules.replace_done"><code class="name">var <span class="ident">replace_done</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SubmitRules.retry"><code class="name">var <span class="ident">retry</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dora.conf.SubmitRules.update"><code class="name">var <span class="ident">update</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dora" href="index.html">dora</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="dora.conf.update_from_args" href="#dora.conf.update_from_args">update_from_args</a></code></li>
<li><code><a title="dora.conf.update_from_hydra" href="#dora.conf.update_from_hydra">update_from_hydra</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dora.conf.DoraConfig" href="#dora.conf.DoraConfig">DoraConfig</a></code></h4>
<ul class="two-column">
<li><code><a title="dora.conf.DoraConfig.dir" href="#dora.conf.DoraConfig.dir">dir</a></code></li>
<li><code><a title="dora.conf.DoraConfig.exclude" href="#dora.conf.DoraConfig.exclude">exclude</a></code></li>
<li><code><a title="dora.conf.DoraConfig.git_save" href="#dora.conf.DoraConfig.git_save">git_save</a></code></li>
<li><code><a title="dora.conf.DoraConfig.grid_package" href="#dora.conf.DoraConfig.grid_package">grid_package</a></code></li>
<li><code><a title="dora.conf.DoraConfig.history" href="#dora.conf.DoraConfig.history">history</a></code></li>
<li><code><a title="dora.conf.DoraConfig.is_excluded" href="#dora.conf.DoraConfig.is_excluded">is_excluded</a></code></li>
<li><code><a title="dora.conf.DoraConfig.rendezvous_file" href="#dora.conf.DoraConfig.rendezvous_file">rendezvous_file</a></code></li>
<li><code><a title="dora.conf.DoraConfig.shared" href="#dora.conf.DoraConfig.shared">shared</a></code></li>
<li><code><a title="dora.conf.DoraConfig.shep" href="#dora.conf.DoraConfig.shep">shep</a></code></li>
<li><code><a title="dora.conf.DoraConfig.use_rendezvous" href="#dora.conf.DoraConfig.use_rendezvous">use_rendezvous</a></code></li>
<li><code><a title="dora.conf.DoraConfig.xps" href="#dora.conf.DoraConfig.xps">xps</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dora.conf.ShepConfig" href="#dora.conf.ShepConfig">ShepConfig</a></code></h4>
<ul class="two-column">
<li><code><a title="dora.conf.ShepConfig.arrays" href="#dora.conf.ShepConfig.arrays">arrays</a></code></li>
<li><code><a title="dora.conf.ShepConfig.by_id" href="#dora.conf.ShepConfig.by_id">by_id</a></code></li>
<li><code><a title="dora.conf.ShepConfig.job_file" href="#dora.conf.ShepConfig.job_file">job_file</a></code></li>
<li><code><a title="dora.conf.ShepConfig.latest_submitit" href="#dora.conf.ShepConfig.latest_submitit">latest_submitit</a></code></li>
<li><code><a title="dora.conf.ShepConfig.orphans" href="#dora.conf.ShepConfig.orphans">orphans</a></code></li>
<li><code><a title="dora.conf.ShepConfig.submitit_folder" href="#dora.conf.ShepConfig.submitit_folder">submitit_folder</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dora.conf.SlurmConfig" href="#dora.conf.SlurmConfig">SlurmConfig</a></code></h4>
<ul class="two-column">
<li><code><a title="dora.conf.SlurmConfig.account" href="#dora.conf.SlurmConfig.account">account</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.array_parallelism" href="#dora.conf.SlurmConfig.array_parallelism">array_parallelism</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.comment" href="#dora.conf.SlurmConfig.comment">comment</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.constraint" href="#dora.conf.SlurmConfig.constraint">constraint</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.cpus_per_gpu" href="#dora.conf.SlurmConfig.cpus_per_gpu">cpus_per_gpu</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.cpus_per_task" href="#dora.conf.SlurmConfig.cpus_per_task">cpus_per_task</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.dependents" href="#dora.conf.SlurmConfig.dependents">dependents</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.exclude" href="#dora.conf.SlurmConfig.exclude">exclude</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.gpus" href="#dora.conf.SlurmConfig.gpus">gpus</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.max_num_timeout" href="#dora.conf.SlurmConfig.max_num_timeout">max_num_timeout</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.mem_per_gpu" href="#dora.conf.SlurmConfig.mem_per_gpu">mem_per_gpu</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.one_task_per_node" href="#dora.conf.SlurmConfig.one_task_per_node">one_task_per_node</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.partition" href="#dora.conf.SlurmConfig.partition">partition</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.qos" href="#dora.conf.SlurmConfig.qos">qos</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.setup" href="#dora.conf.SlurmConfig.setup">setup</a></code></li>
<li><code><a title="dora.conf.SlurmConfig.time" href="#dora.conf.SlurmConfig.time">time</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dora.conf.SubmitRules" href="#dora.conf.SubmitRules">SubmitRules</a></code></h4>
<ul class="">
<li><code><a title="dora.conf.SubmitRules.replace" href="#dora.conf.SubmitRules.replace">replace</a></code></li>
<li><code><a title="dora.conf.SubmitRules.replace_done" href="#dora.conf.SubmitRules.replace_done">replace_done</a></code></li>
<li><code><a title="dora.conf.SubmitRules.retry" href="#dora.conf.SubmitRules.retry">retry</a></code></li>
<li><code><a title="dora.conf.SubmitRules.update" href="#dora.conf.SubmitRules.update">update</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>