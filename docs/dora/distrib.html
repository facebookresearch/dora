<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dora.distrib API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dora.distrib</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from collections import namedtuple
import logging
import os
import random
import subprocess as sp

import submitit
import torch

from .xp import get_xp

logger = logging.getLogger(__name__)


DistribSpec = namedtuple(
    &#34;DistribSpec&#34;, &#34;rank world_size local_rank node_rank num_nodes source&#34;)


def set_distrib_env():
    &#34;&#34;&#34;Calling this function will set the distributed environement
    including the master addr, master port etc. You shouldn&#39;t call
    this if you call `dora.distrib.init`, but it can be useful if you need to let
    some other framework handle the distributed initialization.
    &#34;&#34;&#34;
    spec = get_distrib_spec()
    if spec.world_size == 1 and not os.environ.get(&#39;DORA_FORCE_DISTRIB&#39;):
        return
    if &#39;MASTER_ADDR&#39; not in os.environ:
        assert &#39;SLURM_JOB_NODELIST&#39; in os.environ, &#34;case not handled&#34;
        nodelist = os.environ[&#39;SLURM_JOB_NODELIST&#39;]
        nodes = sp.run(&#39;scontrol show hostnames&#39;.split() + [nodelist],
                       capture_output=True, check=True).stdout.decode().split()
        master_node = nodes[0]
        os.environ[&#39;MASTER_ADDR&#39;] = master_node
    if &#39;MASTER_PORT&#39; not in os.environ:
        xp = get_xp()
        # Note that running twice the same XP on the same node will crash,
        # but that shouldn&#39;t really happen
        seed = xp.sig
        # If we are in a Slurm job, let us use the Slurm job id.
        try:
            env = submitit.JobEnvironment()
        except RuntimeError:
            pass
        else:
            seed += env.job_id
        rng = random.Random(seed)
        master_port = rng.randint(20000, 60000)
        os.environ[&#39;MASTER_PORT&#39;] = str(master_port)
    if &#39;WORLD_SIZE&#39; not in os.environ:
        os.environ[&#39;WORLD_SIZE&#39;] = str(spec.world_size)
        os.environ[&#39;RANK&#39;] = str(spec.rank)
        os.environ[&#39;LOCAL_RANK&#39;] = str(spec.local_rank)


def get_distrib_spec():
    &#34;&#34;&#34;Return information on the distributed setup, i.e. world size, rank etc.
    This can be used even before distributed training is initialized, which is useful for
    PytorchLightning for instance.
    &#34;&#34;&#34;
    if &#39;WORLD_SIZE&#39; in os.environ:
        rank = int(os.environ[&#39;RANK&#39;])
        world_size = int(os.environ[&#39;WORLD_SIZE&#39;])
        if &#39;LOCAL_RANK&#39; in os.environ:
            local_rank = int(os.environ[&#39;LOCAL_RANK&#39;])
        else:
            local_rank = rank
        node_rank = 0
        num_nodes = 1
        source = &#34;env&#34;
    else:
        try:
            env = submitit.JobEnvironment()
        except RuntimeError:
            rank = 0
            world_size = 1
            local_rank = 0
            node_rank = 0
            num_nodes = 1
            source = &#34;empty&#34;
        else:
            rank = env.global_rank
            world_size = env.num_tasks
            local_rank = env.local_rank
            node_rank = env.node
            num_nodes = env.num_nodes
            source = &#34;submitit&#34;
    return DistribSpec(rank, world_size, local_rank, node_rank, num_nodes, source)


def init(backend=&#39;nccl&#39;):
    &#34;&#34;&#34;
    Initialize DDP.
    &#34;&#34;&#34;
    if torch.distributed.is_initialized():
        return
    spec = get_distrib_spec()
    if spec.world_size == 1 and not os.environ.get(&#39;DORA_FORCE_DISTRIB&#39;):
        logger.info(&#34;world_size is 1, skipping init.&#34;)
        return
    xp = get_xp()
    if torch.cuda.is_available():
        torch.cuda.set_device(spec.local_rank)
    else:
        assert backend != &#39;nccl&#39;

    if xp.dora.use_rendezvous:
        init_method = &#39;file://&#39; + os.path.abspath(xp.rendezvous_file)
    else:
        set_distrib_env()
        init_method = &#39;env://&#39;
    torch.distributed.init_process_group(
        backend=backend,
        init_method=init_method,
        world_size=spec.world_size,
        rank=spec.rank)
    logger.info(
        &#34;Distributed init: %d/%d (local %d) from %s&#34;,
        spec.rank, spec.world_size, spec.local_rank, spec.source)
    if xp.dora.use_rendezvous:
        torch.distributed.barrier()
        if rank() == 0:
            # Delete rendez vous file early, let&#39;s hope this doesn&#39;t bug too much.
            xp.rendezvous_file.unlink()


def is_master():
    return rank() == 0


def rank():
    if torch.distributed.is_initialized():
        return torch.distributed.get_rank()
    else:
        return 0


def world_size():
    if torch.distributed.is_initialized():
        return torch.distributed.get_world_size()
    else:
        return 1</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dora.distrib.get_distrib_spec"><code class="name flex">
<span>def <span class="ident">get_distrib_spec</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Return information on the distributed setup, i.e. world size, rank etc.
This can be used even before distributed training is initialized, which is useful for
PytorchLightning for instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_distrib_spec():
    &#34;&#34;&#34;Return information on the distributed setup, i.e. world size, rank etc.
    This can be used even before distributed training is initialized, which is useful for
    PytorchLightning for instance.
    &#34;&#34;&#34;
    if &#39;WORLD_SIZE&#39; in os.environ:
        rank = int(os.environ[&#39;RANK&#39;])
        world_size = int(os.environ[&#39;WORLD_SIZE&#39;])
        if &#39;LOCAL_RANK&#39; in os.environ:
            local_rank = int(os.environ[&#39;LOCAL_RANK&#39;])
        else:
            local_rank = rank
        node_rank = 0
        num_nodes = 1
        source = &#34;env&#34;
    else:
        try:
            env = submitit.JobEnvironment()
        except RuntimeError:
            rank = 0
            world_size = 1
            local_rank = 0
            node_rank = 0
            num_nodes = 1
            source = &#34;empty&#34;
        else:
            rank = env.global_rank
            world_size = env.num_tasks
            local_rank = env.local_rank
            node_rank = env.node
            num_nodes = env.num_nodes
            source = &#34;submitit&#34;
    return DistribSpec(rank, world_size, local_rank, node_rank, num_nodes, source)</code></pre>
</details>
</dd>
<dt id="dora.distrib.init"><code class="name flex">
<span>def <span class="ident">init</span></span>(<span>backend='nccl')</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize DDP.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init(backend=&#39;nccl&#39;):
    &#34;&#34;&#34;
    Initialize DDP.
    &#34;&#34;&#34;
    if torch.distributed.is_initialized():
        return
    spec = get_distrib_spec()
    if spec.world_size == 1 and not os.environ.get(&#39;DORA_FORCE_DISTRIB&#39;):
        logger.info(&#34;world_size is 1, skipping init.&#34;)
        return
    xp = get_xp()
    if torch.cuda.is_available():
        torch.cuda.set_device(spec.local_rank)
    else:
        assert backend != &#39;nccl&#39;

    if xp.dora.use_rendezvous:
        init_method = &#39;file://&#39; + os.path.abspath(xp.rendezvous_file)
    else:
        set_distrib_env()
        init_method = &#39;env://&#39;
    torch.distributed.init_process_group(
        backend=backend,
        init_method=init_method,
        world_size=spec.world_size,
        rank=spec.rank)
    logger.info(
        &#34;Distributed init: %d/%d (local %d) from %s&#34;,
        spec.rank, spec.world_size, spec.local_rank, spec.source)
    if xp.dora.use_rendezvous:
        torch.distributed.barrier()
        if rank() == 0:
            # Delete rendez vous file early, let&#39;s hope this doesn&#39;t bug too much.
            xp.rendezvous_file.unlink()</code></pre>
</details>
</dd>
<dt id="dora.distrib.is_master"><code class="name flex">
<span>def <span class="ident">is_master</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_master():
    return rank() == 0</code></pre>
</details>
</dd>
<dt id="dora.distrib.rank"><code class="name flex">
<span>def <span class="ident">rank</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rank():
    if torch.distributed.is_initialized():
        return torch.distributed.get_rank()
    else:
        return 0</code></pre>
</details>
</dd>
<dt id="dora.distrib.set_distrib_env"><code class="name flex">
<span>def <span class="ident">set_distrib_env</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Calling this function will set the distributed environement
including the master addr, master port etc. You shouldn't call
this if you call <code><a title="dora.distrib.init" href="#dora.distrib.init">init()</a></code>, but it can be useful if you need to let
some other framework handle the distributed initialization.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_distrib_env():
    &#34;&#34;&#34;Calling this function will set the distributed environement
    including the master addr, master port etc. You shouldn&#39;t call
    this if you call `dora.distrib.init`, but it can be useful if you need to let
    some other framework handle the distributed initialization.
    &#34;&#34;&#34;
    spec = get_distrib_spec()
    if spec.world_size == 1 and not os.environ.get(&#39;DORA_FORCE_DISTRIB&#39;):
        return
    if &#39;MASTER_ADDR&#39; not in os.environ:
        assert &#39;SLURM_JOB_NODELIST&#39; in os.environ, &#34;case not handled&#34;
        nodelist = os.environ[&#39;SLURM_JOB_NODELIST&#39;]
        nodes = sp.run(&#39;scontrol show hostnames&#39;.split() + [nodelist],
                       capture_output=True, check=True).stdout.decode().split()
        master_node = nodes[0]
        os.environ[&#39;MASTER_ADDR&#39;] = master_node
    if &#39;MASTER_PORT&#39; not in os.environ:
        xp = get_xp()
        # Note that running twice the same XP on the same node will crash,
        # but that shouldn&#39;t really happen
        seed = xp.sig
        # If we are in a Slurm job, let us use the Slurm job id.
        try:
            env = submitit.JobEnvironment()
        except RuntimeError:
            pass
        else:
            seed += env.job_id
        rng = random.Random(seed)
        master_port = rng.randint(20000, 60000)
        os.environ[&#39;MASTER_PORT&#39;] = str(master_port)
    if &#39;WORLD_SIZE&#39; not in os.environ:
        os.environ[&#39;WORLD_SIZE&#39;] = str(spec.world_size)
        os.environ[&#39;RANK&#39;] = str(spec.rank)
        os.environ[&#39;LOCAL_RANK&#39;] = str(spec.local_rank)</code></pre>
</details>
</dd>
<dt id="dora.distrib.world_size"><code class="name flex">
<span>def <span class="ident">world_size</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def world_size():
    if torch.distributed.is_initialized():
        return torch.distributed.get_world_size()
    else:
        return 1</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dora.distrib.DistribSpec"><code class="flex name class">
<span>class <span class="ident">DistribSpec</span></span>
<span>(</span><span>rank, world_size, local_rank, node_rank, num_nodes, source)</span>
</code></dt>
<dd>
<div class="desc"><p>DistribSpec(rank, world_size, local_rank, node_rank, num_nodes, source)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="dora.distrib.DistribSpec.local_rank"><code class="name">var <span class="ident">local_rank</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
<dt id="dora.distrib.DistribSpec.node_rank"><code class="name">var <span class="ident">node_rank</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 3</p></div>
</dd>
<dt id="dora.distrib.DistribSpec.num_nodes"><code class="name">var <span class="ident">num_nodes</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 4</p></div>
</dd>
<dt id="dora.distrib.DistribSpec.rank"><code class="name">var <span class="ident">rank</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="dora.distrib.DistribSpec.source"><code class="name">var <span class="ident">source</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 5</p></div>
</dd>
<dt id="dora.distrib.DistribSpec.world_size"><code class="name">var <span class="ident">world_size</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dora" href="index.html">dora</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="dora.distrib.get_distrib_spec" href="#dora.distrib.get_distrib_spec">get_distrib_spec</a></code></li>
<li><code><a title="dora.distrib.init" href="#dora.distrib.init">init</a></code></li>
<li><code><a title="dora.distrib.is_master" href="#dora.distrib.is_master">is_master</a></code></li>
<li><code><a title="dora.distrib.rank" href="#dora.distrib.rank">rank</a></code></li>
<li><code><a title="dora.distrib.set_distrib_env" href="#dora.distrib.set_distrib_env">set_distrib_env</a></code></li>
<li><code><a title="dora.distrib.world_size" href="#dora.distrib.world_size">world_size</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dora.distrib.DistribSpec" href="#dora.distrib.DistribSpec">DistribSpec</a></code></h4>
<ul class="two-column">
<li><code><a title="dora.distrib.DistribSpec.local_rank" href="#dora.distrib.DistribSpec.local_rank">local_rank</a></code></li>
<li><code><a title="dora.distrib.DistribSpec.node_rank" href="#dora.distrib.DistribSpec.node_rank">node_rank</a></code></li>
<li><code><a title="dora.distrib.DistribSpec.num_nodes" href="#dora.distrib.DistribSpec.num_nodes">num_nodes</a></code></li>
<li><code><a title="dora.distrib.DistribSpec.rank" href="#dora.distrib.DistribSpec.rank">rank</a></code></li>
<li><code><a title="dora.distrib.DistribSpec.source" href="#dora.distrib.DistribSpec.source">source</a></code></li>
<li><code><a title="dora.distrib.DistribSpec.world_size" href="#dora.distrib.DistribSpec.world_size">world_size</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>